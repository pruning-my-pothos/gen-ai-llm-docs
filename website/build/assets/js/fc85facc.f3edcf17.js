"use strict";(self.webpackChunknnlp_website=self.webpackChunknnlp_website||[]).push([[6898],{28453(e,l,n){n.d(l,{R:()=>i,x:()=>r});var o=n(96540);const t={},a=o.createContext(t);function i(e){const l=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(l):{...l,...e}},[l,e])}function r(e){let l;return l=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),o.createElement(a.Provider,{value:l},e.children)}},77224(e,l,n){n.r(l),n.d(l,{assets:()=>s,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"06-frameworks-and-tooling/03-local-inference","title":"Local Inference: Ollama","description":"When working with Red Zone data (PII, secrets, core IP), you cannot send code to the cloud. Local inference allows you to execute NNLP safely on your own hardware.","source":"@site/../docs/06-frameworks-and-tooling/03-local-inference.md","sourceDirName":"06-frameworks-and-tooling","slug":"/06-frameworks-and-tooling/03-local-inference","permalink":"/NNLP/docs/06-frameworks-and-tooling/03-local-inference","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"nnlp","permalink":"/NNLP/docs/tags/nnlp"},{"inline":true,"label":"tooling","permalink":"/NNLP/docs/tags/tooling"},{"inline":true,"label":"local-llm","permalink":"/NNLP/docs/tags/local-llm"},{"inline":true,"label":"privacy","permalink":"/NNLP/docs/tags/privacy"},{"inline":true,"label":"ollama","permalink":"/NNLP/docs/tags/ollama"}],"version":"current","lastUpdatedAt":1766231902000,"frontMatter":{"title":"Local Inference: Ollama","archetype":"pattern","status":"active","owner":"Shailesh (Shaily)","maintainer":"Shailesh (Shaily)","version":"0.1.0","tags":["nnlp","tooling","local-llm","privacy","ollama"],"last_reviewed":"2025-12-20"},"sidebar":"mainSidebar","previous":{"title":"04-deployment-considerations","permalink":"/NNLP/docs/06-frameworks-and-tooling/03-local-first/04-deployment-considerations"},"next":{"title":"00-guardrails-index","permalink":"/NNLP/docs/07-guardrails-and-governance/00-guardrails-index"}}');var t=n(74848),a=n(28453);const i={title:"Local Inference: Ollama",archetype:"pattern",status:"active",owner:"Shailesh (Shaily)",maintainer:"Shailesh (Shaily)",version:"0.1.0",tags:["nnlp","tooling","local-llm","privacy","ollama"],last_reviewed:"2025-12-20"},r="Local Inference: Ollama",s={},c=[{value:"Overview",id:"overview",level:2},{value:"Adjusting NNLP for Local Models",id:"adjusting-nnlp-for-local-models",level:2},{value:"1. Simplify the Logic",id:"1-simplify-the-logic",level:3},{value:"2. Increase Explicitness",id:"2-increase-explicitness",level:3},{value:"Setup Guide (Ollama)",id:"setup-guide-ollama",level:2},{value:"1. Install &amp; Pull",id:"1-install--pull",level:3},{value:"2. Connect to IDE",id:"2-connect-to-ide",level:3},{value:"3. The &quot;System Prompt&quot; Trick",id:"3-the-system-prompt-trick",level:3},{value:"Visual: The Air Gap",id:"visual-the-air-gap",level:2},{value:"Last Reviewed / Last Updated",id:"last-reviewed--last-updated",level:2}];function d(e){const l={admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(l.header,{children:(0,t.jsx)(l.h1,{id:"local-inference-ollama",children:"Local Inference: Ollama"})}),"\n",(0,t.jsx)(l.admonition,{title:"Why Local?",type:"info",children:(0,t.jsxs)(l.p,{children:["When working with ",(0,t.jsx)(l.strong,{children:"Red Zone"})," data (PII, secrets, core IP), you cannot send code to the cloud. Local inference allows you to execute NNLP safely on your own hardware."]})}),"\n",(0,t.jsx)(l.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(l.p,{children:["Tools like ",(0,t.jsx)(l.strong,{children:"Ollama"})," or ",(0,t.jsx)(l.strong,{children:"LM Studio"})," allow you to run models like Llama 3, Mistral, or DeepSeek Coder locally."]}),"\n",(0,t.jsxs)(l.p,{children:[(0,t.jsx)(l.strong,{children:"Trade-off"}),': Local models are generally "dumber" than GPT-4 or Claude 3.5.\n',(0,t.jsx)(l.strong,{children:"NNLP Implication"}),": You must write ",(0,t.jsx)(l.strong,{children:"tighter constraints"})," and ",(0,t.jsx)(l.strong,{children:"simpler sentences"})," to get good results."]}),"\n",(0,t.jsx)(l.hr,{}),"\n",(0,t.jsx)(l.h2,{id:"adjusting-nnlp-for-local-models",children:"Adjusting NNLP for Local Models"}),"\n",(0,t.jsx)(l.p,{children:"Local models struggle with complex, multi-step reasoning. You must break the loop down further."}),"\n",(0,t.jsx)(l.h3,{id:"1-simplify-the-logic",children:"1. Simplify the Logic"}),"\n",(0,t.jsx)(l.p,{children:'Instead of one big "Refactor this" prompt, split it:'}),"\n",(0,t.jsxs)(l.ol,{children:["\n",(0,t.jsx)(l.li,{children:'"Add types to function A."'}),"\n",(0,t.jsx)(l.li,{children:'"Add types to function B."'}),"\n",(0,t.jsx)(l.li,{children:'"Update the export."'}),"\n"]}),"\n",(0,t.jsx)(l.h3,{id:"2-increase-explicitness",children:"2. Increase Explicitness"}),"\n",(0,t.jsx)(l.p,{children:"Cloud models infer context. Local models need it spelled out."}),"\n",(0,t.jsxs)(l.ul,{children:["\n",(0,t.jsxs)(l.li,{children:[(0,t.jsx)(l.strong,{children:"Bad"}),': "Use standard error handling."']}),"\n",(0,t.jsxs)(l.li,{children:[(0,t.jsx)(l.strong,{children:"Good"}),': "Wrap the database call in a try/catch block. Log the error to stderr."']}),"\n"]}),"\n",(0,t.jsx)(l.hr,{}),"\n",(0,t.jsx)(l.h2,{id:"setup-guide-ollama",children:"Setup Guide (Ollama)"}),"\n",(0,t.jsx)(l.h3,{id:"1-install--pull",children:"1. Install & Pull"}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-bash",children:"brew install ollama\nollama pull llama3\n"})}),"\n",(0,t.jsx)(l.h3,{id:"2-connect-to-ide",children:"2. Connect to IDE"}),"\n",(0,t.jsxs)(l.p,{children:["Most IDEs (VS Code, Cursor) allow you to point the API endpoint to ",(0,t.jsx)(l.code,{children:"localhost:11434"}),"."]}),"\n",(0,t.jsx)(l.h3,{id:"3-the-system-prompt-trick",children:'3. The "System Prompt" Trick'}),"\n",(0,t.jsxs)(l.p,{children:["Local models often forget instructions. Paste your ",(0,t.jsx)(l.strong,{children:"Delegation Contract"})," at the top of ",(0,t.jsx)(l.em,{children:"every"})," prompt, or use a custom Modelfile."]}),"\n",(0,t.jsx)(l.p,{children:(0,t.jsx)(l.strong,{children:"Example Modelfile:"})}),"\n",(0,t.jsx)(l.pre,{children:(0,t.jsx)(l.code,{className:"language-dockerfile",children:'FROM llama3\nSYSTEM "You are a coding assistant. You must follow the constraints provided in the prompt exactly. Do not be conversational."\n'})}),"\n",(0,t.jsx)(l.hr,{}),"\n",(0,t.jsx)(l.h2,{id:"visual-the-air-gap",children:"Visual: The Air Gap"}),"\n",(0,t.jsx)(l.mermaid,{value:"flowchart LR\n    User[Developer] --\x3e|Prompt| Local[Local LLM (GPU)]\n    Local --\x3e|Code| Repo[Local Repo]\n\n    subgraph Internet [The Internet]\n        Cloud[OpenAI / Anthropic]\n    end\n\n    Local -.->|\u274c No Connection| Cloud\n\n    classDef safe fill:#E6F7FF,stroke:#1B75BB,color:#0F1F2E;\n    class User,Local,Repo safe;"}),"\n",(0,t.jsx)(l.hr,{}),"\n",(0,t.jsx)(l.h2,{id:"last-reviewed--last-updated",children:"Last Reviewed / Last Updated"}),"\n",(0,t.jsxs)(l.ul,{children:["\n",(0,t.jsx)(l.li,{children:"Last reviewed: 2025-12-20"}),"\n",(0,t.jsx)(l.li,{children:"Version: 0.1.0"}),"\n"]})]})}function h(e={}){const{wrapper:l}={...(0,a.R)(),...e.components};return l?(0,t.jsx)(l,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);