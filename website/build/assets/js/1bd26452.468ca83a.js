"use strict";(self.webpackChunknnlp_website=self.webpackChunknnlp_website||[]).push([[9543],{269(e,t,n){n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>r,default:()=>c,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"07-guardrails-and-governance/threat-model-lite","title":"Threat Model Lite","description":"Traditional threat modeling takes days. \\"Threat Model Lite\\" takes 10 minutes. It focuses exclusively on the unique risks introduced by AI components.","source":"@site/../docs/07-guardrails-and-governance/threat-model-lite.md","sourceDirName":"07-guardrails-and-governance","slug":"/07-guardrails-and-governance/threat-model-lite","permalink":"/NNLP/docs/07-guardrails-and-governance/threat-model-lite","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"nnlp","permalink":"/NNLP/docs/tags/nnlp"},{"inline":true,"label":"security","permalink":"/NNLP/docs/tags/security"},{"inline":true,"label":"risk","permalink":"/NNLP/docs/tags/risk"},{"inline":true,"label":"threat-modeling","permalink":"/NNLP/docs/tags/threat-modeling"},{"inline":true,"label":"method","permalink":"/NNLP/docs/tags/method"}],"version":"current","lastUpdatedAt":1766231902000,"frontMatter":{"title":"Threat Model Lite","archetype":"method","status":"active","owner":"Shailesh (Shaily)","maintainer":"Shailesh (Shaily)","version":"0.1.0","tags":["nnlp","security","risk","threat-modeling","method"],"last_reviewed":"2025-12-20"},"sidebar":"mainSidebar","previous":{"title":"Guardrails and Governance","permalink":"/NNLP/docs/07-guardrails-and-governance/guardrails-index"},"next":{"title":"Evaluation Overview","permalink":"/NNLP/docs/08-evaluation/00-eval-overview"}}');var s=n(74848),l=n(28453);const a={title:"Threat Model Lite",archetype:"method",status:"active",owner:"Shailesh (Shaily)",maintainer:"Shailesh (Shaily)",version:"0.1.0",tags:["nnlp","security","risk","threat-modeling","method"],last_reviewed:"2025-12-20"},r="Threat Model Lite",o={},d=[{value:"Overview",id:"overview",level:2},{value:"The 3-Question Assessment",id:"the-3-question-assessment",level:2},{value:"1. What context does the AI have?",id:"1-what-context-does-the-ai-have",level:3},{value:"2. What can the AI output touch?",id:"2-what-can-the-ai-output-touch",level:3},{value:"3. Who controls the input?",id:"3-who-controls-the-input",level:3},{value:"Visual: Risk Flow",id:"visual-risk-flow",level:2},{value:"Common AI Threats &amp; Mitigations",id:"common-ai-threats--mitigations",level:2},{value:"When to Use This",id:"when-to-use-this",level:2},{value:"Checklist: Is It Safe?",id:"checklist-is-it-safe",level:2},{value:"Next Step",id:"next-step",level:2},{value:"Last Reviewed / Last Updated",id:"last-reviewed--last-updated",level:2}];function h(e){const t={admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",mermaid:"mermaid",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"threat-model-lite",children:"Threat Model Lite"})}),"\n",(0,s.jsx)(t.admonition,{title:"Purpose",type:"info",children:(0,s.jsx)(t.p,{children:'Traditional threat modeling takes days. "Threat Model Lite" takes 10 minutes. It focuses exclusively on the unique risks introduced by AI components.'})}),"\n",(0,s.jsx)(t.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(t.p,{children:"AI introduces new attack vectors that traditional security reviews often miss:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Prompt Injection"}),": Users tricking the model into ignoring instructions."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Hallucination"}),": The model confidently inventing libraries or facts."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Data Leakage"}),": The model repeating sensitive data from its context window."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"You cannot patch these risks later. You must design for them."}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"the-3-question-assessment",children:"The 3-Question Assessment"}),"\n",(0,s.jsx)(t.p,{children:"Before shipping any AI-assisted feature, answer these three questions:"}),"\n",(0,s.jsx)(t.h3,{id:"1-what-context-does-the-ai-have",children:"1. What context does the AI have?"}),"\n",(0,s.jsxs)(t.p,{children:["Does it see PII? Secrets? Internal docs?\n",(0,s.jsx)(t.em,{children:"Risk: Data Leakage."})]}),"\n",(0,s.jsx)(t.h3,{id:"2-what-can-the-ai-output-touch",children:"2. What can the AI output touch?"}),"\n",(0,s.jsxs)(t.p,{children:["Does it write SQL? HTML? Shell commands?\n",(0,s.jsx)(t.em,{children:"Risk: Injection / Execution."})]}),"\n",(0,s.jsx)(t.h3,{id:"3-who-controls-the-input",children:"3. Who controls the input?"}),"\n",(0,s.jsxs)(t.p,{children:["Is the prompt fixed by us, or can users type into it?\n",(0,s.jsx)(t.em,{children:"Risk: Prompt Injection / Jailbreaking."})]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"visual-risk-flow",children:"Visual: Risk Flow"}),"\n",(0,s.jsx)(t.mermaid,{value:"flowchart LR\n    User[User Input] --\x3e|Injection?| AI[AI Model]\n    Context[Private Data] --\x3e|Leakage?| AI\n    AI --\x3e|Hallucination?| Output[System Action]\n\n    classDef risk fill:#FFE6E6,stroke:#D32F2F,color:#0F1F2E;\n    class User,Context,Output risk;\n    class AI fill:#E6F7FF,stroke:#1B75BB,color:#0F1F2E;"}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"common-ai-threats--mitigations",children:"Common AI Threats & Mitigations"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Threat"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Description"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Mitigation"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Prompt Injection"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"User overrides system instructions."}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Treat all AI output as untrusted user input. Sanitize it."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Hallucinated Packages"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"AI suggests a malicious package."}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Verify existence and reputation of all dependencies manually."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Insecure Code"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"AI writes vulnerable SQL/Regex."}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Use static analysis (SAST) on generated code."})]})]})]}),"\n",(0,s.jsx)(t.admonition,{title:"The Golden Rule",type:"danger",children:(0,s.jsx)(t.p,{children:"Never allow an LLM to execute actions (delete, buy, ban) without a human confirmation step or strict deterministic validation."})}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"when-to-use-this",children:"When to Use This"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Every time"})," you add an AI feature to production."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Every time"})," you change the data access level of an AI tool."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Every time"})," you switch models (safety profiles vary)."]}),"\n"]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"checklist-is-it-safe",children:"Checklist: Is It Safe?"}),"\n",(0,s.jsxs)(t.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(t.li,{className:"task-list-item",children:[(0,s.jsx)(t.input,{type:"checkbox",disabled:!0})," ","Inputs are sanitized before reaching the prompt."]}),"\n",(0,s.jsxs)(t.li,{className:"task-list-item",children:[(0,s.jsx)(t.input,{type:"checkbox",disabled:!0})," ","Outputs are validated/sanitized before execution."]}),"\n",(0,s.jsxs)(t.li,{className:"task-list-item",children:[(0,s.jsx)(t.input,{type:"checkbox",disabled:!0})," ","The model has the minimum necessary context (Principle of Least Privilege)."]}),"\n",(0,s.jsxs)(t.li,{className:"task-list-item",children:[(0,s.jsx)(t.input,{type:"checkbox",disabled:!0})," ","A human is in the loop for high-stakes actions."]}),"\n"]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"next-step",children:"Next Step"}),"\n",(0,s.jsxs)(t.p,{children:["Apply this method using the template:\n",(0,s.jsx)(t.strong,{children:(0,s.jsx)(t.code,{children:"docs/09-templates/threat-model-lite-template.md"})})]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"last-reviewed--last-updated",children:"Last Reviewed / Last Updated"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Last reviewed: 2025-12-20"}),"\n",(0,s.jsx)(t.li,{children:"Version: 0.1.0"}),"\n"]})]})}function c(e={}){const{wrapper:t}={...(0,l.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},28453(e,t,n){n.d(t,{R:()=>a,x:()=>r});var i=n(96540);const s={},l=i.createContext(s);function a(e){const t=i.useContext(l);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(l.Provider,{value:t},e.children)}}}]);