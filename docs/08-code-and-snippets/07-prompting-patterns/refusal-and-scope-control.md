---
title: "Refusal and Scope Control"
archetype: "code-snippet"
status: "active"
owner: "NNLP"
maintainer: "NNLP"
version: "1.0.0"
tags: ["prompt-engineering", "safety", "scope-control", "refusal"]
last_reviewed: "2025-12-31"
---

# Refusal and Scope Control

Controlling what your LLM application will and will not answer is crucial for safety, reliability, and managing user expectations. By explicitly instructing the model on its scope and how to refuse out-of-scope inquiries, you prevent hallucinations, irrelevant responses, and potential misuse.

:::info[The Goal: Stay within Boundaries]
The objective is to confine the model's responses to its intended domain and gracefully handle queries that fall outside its defined capabilities or are inappropriate.
:::

---

## 1. Defining Scope in the System Prompt

The primary method for controlling scope is to embed clear instructions within the system prompt. These instructions should define the model's purpose, what it *can* do, and what it *cannot* do.

```python
# Example System Prompt for a specialized assistant
system_prompt_scoped = """
You are a highly knowledgeable technical support assistant for 'QuantumFlow Cloud Services'.
- Your expertise is limited to QuantumFlow's API documentation, pricing, and service status.
- You MUST only provide information directly related to QuantumFlow.
- If a question is outside the scope of QuantumFlow services, you MUST politely refuse to answer.
- Do NOT engage in general conversation, philosophical discussions, or provide information on other cloud providers.
"""
```

---

## 2. Enforcing Hard Refusal Statements

To make refusals consistent and predictable, you can instruct the model to use a specific, pre-defined refusal phrase for out-of-scope questions.

```python
# Example System Prompt with a hard refusal statement
system_prompt_hard_refusal = """
You are a technical support assistant for 'QuantumFlow Cloud Services'.
- Your expertise is strictly limited to QuantumFlow's products.
- If a question is not about QuantumFlow, you MUST respond EXACTLY with:
  'I am sorry, but my knowledge is restricted to QuantumFlow Cloud Services. Please ask me a question about QuantumFlow.'
- Do not deviate from this refusal statement.
"""
```

---

## 3. Detecting Refusals in Model Output

After receiving a response from the LLM, your application can check for these refusal phrases. This allows your application to handle the refusal gracefully, for example, by logging it, displaying a custom message to the user, or escalating to human support.

```python
from typing import List, Dict
import re

def detect_refusal(model_output: str, refusal_phrases: List[str]) -> bool:
    """
    Checks if the model's output contains any of the predefined refusal phrases.

    Args:
        model_output: The text generated by the LLM.
        refusal_phrases: A list of exact phrases the model should use for refusal.

    Returns:
        True if a refusal phrase is detected, False otherwise.
    """
    # Create a regex pattern to match any of the phrases, case-insensitively
    pattern = r"|".join(map(re.escape, refusal_phrases))
    return bool(re.search(pattern, model_output, re.IGNORECASE))

# --- Example Usage ---
known_refusals = ["I am sorry, but my knowledge is restricted to QuantumFlow Cloud Services.",
                  "I cannot assist with that as it is outside my scope."]

# Simulate LLM outputs
output_1 = "Sure, I can help you with QuantumFlow's pricing."
output_2 = "I am sorry, but my knowledge is restricted to QuantumFlow Cloud Services. Please ask me a question about QuantumFlow."
output_3 = "Tell me more about AWS. (This should be refused by the model itself, not detected here directly)"

print(f"Output 1 refusal detected: {detect_refusal(output_1, known_refusals)}")
print(f"Output 2 refusal detected: {detect_refusal(output_2, known_refusals)}")
print(f"Output 3 refusal detected (false positive possible if model doesn't use exact phrase): {detect_refusal(output_3, known_refusals)}")

# In a real app:
# llm_response = call_llm(messages) # assuming call_llm from previous guides
# if detect_refusal(llm_response, known_refusals):
#     print("Application detected a refusal. Redirecting user.")
# else:
#     print(f"LLM responded: {llm_response}")
```

---

:::warning[Prompt Injection Risk]
While strong refusal instructions are effective, they can be vulnerable to [Prompt Injection](../06-context-hygiene/prompt-injection-red-flags.md). Malicious users might try to bypass your refusal instructions. A layered defense (e.g., input scanning + strong system prompts) is always recommended.
:::
