---
title: "curl Chat Completions for Local LLMs"
archetype: "code-snippet"
status: "active"
owner: "NNLP"
maintainer: "NNLP"
version": "1.0.0"
tags: ["cli", "curl", "api", "local-llm", "openai-compatible"]
last_reviewed: "2025-12-31"
---

# `curl` Chat Completions for Local LLMs

The `curl` command-line tool is invaluable for quick testing, debugging, and understanding the raw API interactions with your local LLM servers. This guide provides a complete example of how to make a chat completion request to an OpenAI-compatible local LLM using `curl`.

:::info[The Goal: Raw API Interaction]
The objective is to demonstrate the fundamental HTTP request structure for LLM chat completions, allowing you to manually verify server functionality and explore API responses directly from your terminal.
:::

---

## 1. Prerequisites

-   An OpenAI-compatible local LLM server running (e.g., Ollama, LM Studio, vLLM).
-   A model loaded on the server (e.g., `llama3` on Ollama, `phi-3-mini` on LM Studio).
-   `curl` installed on your system (usually pre-installed on Linux/macOS).

---

## 2. The `chat/completions` Endpoint

OpenAI-compatible servers typically expose a `/v1/chat/completions` endpoint. This endpoint accepts `POST` requests with a JSON payload defining the messages and model parameters.

### Request Body Structure (JSON)

```json
{
  "model": "your-model-name",        // The name of the model on your local server
  "messages": [
    {"role": "system", "content": "Your system prompt."},
    {"role": "user", "content": "Your user query."},
    // {"role": "assistant", "content": "Previous assistant response."}, // Optional
    // {"role": "user", "content": "Follow-up query."}
  ],
  "temperature": 0.7,                // Controls randomness (0.0 for deterministic)
  "max_tokens": 150,                 // Maximum number of tokens to generate
  "stream": false                    // Set to true for streaming responses
}
```

---

## 3. Full `curl` Example (Non-Streaming)

This command sends a `POST` request to a local Ollama server (running on port `11434`) with a simple chat prompt and receives a full JSON response.

```bash
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d 
    "{
    "model": "llama3",
    "messages": [
      { "role": "system", "content": "You are a concise, helpful assistant." },
      { "role": "user", "content": "What is the capital of France?" }
    ],
    "temperature": 0.0,
    "max_tokens": 50,
    "stream": false
  }"
```

### Expected JSON Response

The server will return a JSON object similar to this:

```json
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "created": 1700000000,
  "model": "llama3",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Paris is the capital of France."
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 20,
    "completion_tokens": 7,
    "total_tokens": 27
  }
}
```
The actual text generated by the LLM is found in `choices[0].message.content`.

---

## 4. Full `curl` Example (Streaming)

For a more interactive experience, you can request a streaming response. This makes the LLM send tokens as they are generated, improving perceived latency.

```bash
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d 
    "{
    "model": "llama3",
    "messages": [
      { "role": "system", "content": "You are a concise, helpful assistant." },
      { "role": "user", "content": "What is the capital of France?" }
    ],
    "temperature": 0.0,
    "max_tokens": 50,
    "stream": true
  }"
```

The response will be a series of JSON objects, each representing a "chunk" of the message.
*Related Guide: [Streaming Basics](./streaming-basics.md)*

---

:::tip[Combine with `jq` for Parsing]
The raw `curl` output can be verbose. Use `jq` to easily extract specific fields from the JSON response, making debugging much faster.

```bash
# Extract only the assistant's content
curl -s -X POST ... -d '{...}' | jq -r '.choices[0].message.content'
```
*Related Guide: [CLI Power Tools: curl, jq, and ripgrep](./../03-cli-and-shell-essentials/curl-jq-ripgrep.md)*
:::